{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "from chardet import detect\n",
    "\n",
    "# get file encoding type\n",
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']\n",
    "\n",
    "\n",
    "def correctTxtEncoding(filename, encoding_to='UTF-8'):\n",
    "    from_codec = get_encoding_type(filename)\n",
    "    temp_filename = filename[:-4]+\"temp.txt\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding=from_codec) as fr:\n",
    "            with open(temp_filename, 'w', encoding=encoding_to) as fw:\n",
    "                for line in fr:\n",
    "                    fw.write(line[:-1]+'\\r\\n')\n",
    "        os.remove(filename) # remove old encoding file\n",
    "        os.rename(temp_filename, filename) # rename new encoding\n",
    "    except UnicodeDecodeError:\n",
    "        print('Decode Error')\n",
    "    except UnicodeEncodeError:\n",
    "        print('Encode Error')\n",
    "\n",
    "def clean_text(string):\n",
    "    pattern = '(page|PAGE|Page)(\\s+\\|\\s+)([0-9]+)(.*)$'\n",
    "    output_cleaned = re.sub('\\s$', '', string, flags=re.MULTILINE)\n",
    "    p=re.compile(pattern,re.MULTILINE)\n",
    "    output_cleaned = p.sub(\" \",output_cleaned)\n",
    "    return output_cleaned\n",
    "\n",
    "def merge_texts(texts):\n",
    "    merged_text = ''\n",
    "    for text in tqdm(texts):\n",
    "        correctTxtEncoding(text)\n",
    "        with open(text,'r',encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                merged_text += line[:-1]+'\\n'\n",
    "    merged_text_cleaned = clean_text(merged_text)\n",
    "    return merged_text_cleaned\n",
    "\n",
    "def get_all_txt_files():\n",
    "    path = os.path.abspath(os.path.join(\"../\", os.pardir))+'\\Data\\**\\*.txt';\n",
    "    files = glob.glob(path, recursive=True)\n",
    "    return files\n",
    "\n",
    "def get_files_in_data_folder(folder):\n",
    "    path = os.path.abspath(os.path.join(\"../\", os.pardir))+ '\\Data\\\\'+ folder + '\\*.txt';\n",
    "    files = glob.glob(path, recursive=True)\n",
    "    return files\n",
    "\n",
    "def get_files_by_author(author):\n",
    "    path = os.path.abspath(os.path.join(\"../\", os.pardir))+ '\\Data\\**\\*_' + author + '.txt';\n",
    "    files = glob.glob(path, recursive=True)\n",
    "    return files\n",
    "\n",
    "def get_train_test_validation(txt_file, train=0.70, test=0.20, val=0.10):\n",
    "    train_doc =[]\n",
    "    test_doc = []\n",
    "    val_doc =[]\n",
    "    with open(txt_file,'r', encoding='UTF-8') as f:\n",
    "        file_input=f.readlines()\n",
    "\n",
    "    count = 0\n",
    "    for cnt, line in enumerate(file_input):\n",
    "            if cnt <= len(file_input)*train:\n",
    "                train_doc.append(line)\n",
    "            elif (cnt > len(file_input)*train and cnt < len(file_input)*(train+test)):\n",
    "                test_doc.append(line)\n",
    "            else:\n",
    "                val_doc.append(line)\n",
    "\n",
    "    ## Write to file\n",
    "    f = open(txt_file[:-4]+'_train.txt', \"w+\", encoding='UTF-8')\n",
    "    count = 0\n",
    "    for line in train_doc:\n",
    "        count=count+1\n",
    "        f.write(str(line))\n",
    "        f.write(\"\\n\")  \n",
    "    f.close()\n",
    "    print(\"Training lines:\\t\",count)\n",
    "    \n",
    "    ## Write to file\n",
    "    f = open(txt_file[:-4]+'_test.txt', \"w+\", encoding='UTF-8')\n",
    "    count = 0\n",
    "    for line in train_doc:\n",
    "        count=count+1\n",
    "        f.write(str(line))\n",
    "        f.write(\"\\n\")  \n",
    "    f.close()\n",
    "    print(\"Testing lines:\\t\",count)\n",
    "\n",
    "    ## Write to file\n",
    "    f = open(txt_file[:-4]+'_val.txt',\"w+\", encoding='UTF-8')\n",
    "    count = 0\n",
    "    for line in val_doc:\n",
    "        count=count+1\n",
    "        f.write(str(line))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    f.close()\n",
    "    print(\"Validation lines:\\t\",count)\n",
    "    \n",
    "def save_output_file(path_from_output, file_name, data):\n",
    "    path = os.path.abspath(os.path.join(\"../\", os.pardir))+ '\\Data\\Outputs\\\\'+path_from_output + file_name;\n",
    "    with open(path, \"w\", encoding='UTF-8') as file:\n",
    "        file.write(data)\n",
    "    print(\"File saved at:\\t\", path)\n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "# all_text_files = get_all_txt_files()\n",
    "# harry_potter_texts = merge_texts(get_files_in_data_folder(\"Harry_Potter\"))\n",
    "# print(\"Cleaned Harry Potter Text:\\n\\n\\n\" + harry_potter_texts[:2000]+\"....\\n\\n\")\n",
    "# stephen_king_texts = merge_texts(get_files_by_author(\"Stephen_King\"))\n",
    "# print(\"Cleaned Stephen Text:\\n\\n\\n\" + stephen_king_texts[:2000]+\"....\\n\\n\")\n",
    "# horror_movie_transcripts = merge_texts(get_files_in_data_folder(\"Horror_Movie_Transcripts\"))\n",
    "# print(\"Cleaned Horror Movie Transcripts:\\n\\n\\n\" + horror_movie_transcripts[:2000]+\"....\\n\\n\")\n",
    "# public_domain_texts = merge_texts(get_files_in_data_folder(\"Public_Domain_Horror_Novels\"))\n",
    "# print(\"Cleaned Public Domain Horror Novels:\\n\\n\\n\" + public_domain_texts[:2000]+\"....\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [11:19<00:00, 75.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at:\t C:\\Users\\Carson\\OneDrive\\Desktop\\Programming\\Projects\\Epic\\Data\\Outputs\\Stephen_King_Playground\\merged_Stephen_King.txt\n",
      "Training lines:\t 80009\n",
      "Testing lines:\t 80009\n",
      "Validation lines:\t 11429\n"
     ]
    }
   ],
   "source": [
    "get_train_test_validation(save_output_file('Stephen_King_Playground\\\\', \n",
    "                 'merged_Stephen_King.txt', \n",
    "                 merge_texts(get_files_by_author(\"Stephen_King\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

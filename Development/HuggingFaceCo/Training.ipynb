{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"QHIoBCmGi-SU","executionInfo":{"status":"ok","timestamp":1603316736378,"user_tz":360,"elapsed":13287,"user":{"displayName":"Carson Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_0oYvUVBGdb2NYg8kqCEFgYdcDwse6s0G_88RxA=s64","userId":"00108991392718208205"}},"outputId":"ce25de10-2138-4011-8e97-a0b3e9862c83","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install transformers\n","# !pip install wandb\n","# import wafndb\n","# wandb.login()\n","\n","# Optional: log both gradients and parameters\n","# %env WANDB_WATCH=all\n","\n","\n","import ipywidgets\n","from IPython import display\n","import os\n","\n","import torch\n","\n","from transformers import TextDataset,DataCollatorForLanguageModeling\n","from transformers import GPT2Model, GPT2Config\n","from transformers import AutoTokenizer, AutoModelWithLMHead\n","from transformers import Trainer, TrainingArguments\n","\n","def load_dataset(train_path,test_path,tokenizer):\n","    train_dataset = TextDataset(\n","          tokenizer=tokenizer,\n","          file_path=train_path,\n","          block_size=128)\n","\n","    test_dataset = TextDataset(\n","          tokenizer=tokenizer,\n","          file_path=test_path,\n","          block_size=128)\n","\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=False,\n","    )\n","    return train_dataset,test_dataset,data_collator\n","\n","\n","display.clear_output()\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","\n","if gpu_info.find('failed') >= 0 and ram_gb < 30:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  try:\n","      if gpu_info.find('failed') < 0:\n","         print(gpu_info)\n","  except:\n","    display.clear_output()\n","  finally:\n","    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","    print('You are using a high-RAM runtime!')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Your runtime has 38.0 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7dtD5xtJicHv","outputId":"edb99420-fc68-45c2-839b-2754e959c753","colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["parent_directory = '/content/drive/My Drive/CSCI 470 Project/Epic/'\n","checkpoint_path = 'Models/gpt2_medium_Stephen_King/checkpoint-000'\n","epochs = 3\n","batch_size=5\n","# wandb.init(config={\"epochs\": epochs, \"batch_size\": batch_size})\n","\n","# Initializing a GPT2 configuration\n","configuration = GPT2Config()\n","# Initializing a model from the configuration\n","model = GPT2Model(configuration)\n","# Accessing the model configuration\n","configuration = model.config\n","# print(configuration)\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","# # Download model and configuration from S3 and cache.\n","model = AutoModelWithLMHead.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n","\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"pranavpsv/gpt2-genre-story-generator\")\n","\n","# model = AutoModelWithLMHead.from_pretrained(\"pranavpsv/gpt2-genre-story-generator\")\n","# # Example of usage\n","# from transformers import pipeline\n","\n","# story_gen = pipeline(\"text-generation\", \"pranavpsv/gpt2-genre-story-generator\")\n","# print(story_gen(\"<BOS> <superhero> Batman\"))\n","\n","train_path = parent_directory + 'Data/Cleaned_UTF8/merged_Stephen_King_train.txt';\n","test_path = parent_directory + 'Data/Cleaned_UTF8/merged_Stephen_King_test.txt';\n","output_path = parent_directory + 'Models/gpt2_small_Stephen_King'\n","\n","train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)\n","training_args = TrainingArguments(\n","  run_name=\"gpt2-small-generator_initial_Stephen_King_texts\",\n","  output_dir=output_path, #The output directory\n","  overwrite_output_dir=True, #overwrite the content of the output directory, set to true if continuing training\n","  num_train_epochs=epochs, # number of training epochs\n","  per_device_train_batch_size=batch_size, # batch size for training\n","  per_device_eval_batch_size=int(batch_size/2),  # batch size for evaluation\n","  eval_steps = 100, # Number of update steps between two evaluations.\n","  save_steps = 200, # after # steps model is saved\n","  warmup_steps=200,# number of warmup steps for learning rate scheduler\n","  evaluation_strategy=\"steps\",\n","  logging_steps = 200,\n","#   logging_dir=parent_directory+\"Models/gpt2_small_Stephen_King/logs\",\n","  do_predict=True\n","  )\n","\n","trainer = Trainer(\n","  model=model,\n","  args=training_args,\n","  data_collator=data_collator,\n","  train_dataset=train_dataset,\n","  eval_dataset=test_dataset\n",")\n","\n","try:\n","  display.clear_output()\n","  print(\"Train Path:\\t\", train_path)\n","  print(\"Test Path:\\t\", test_path)\n","  print(\"Output Directory Path:\\t\", output_path)\n","  trainer.train()\n","\n","finally:\n","  print(\"Exiting Training\")\n","  trainer.save_model()\n","  drive.flush_and_unmount()\n","  print('All changes made in this colab session should now be visible in Drive.')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Path:\t /content/drive/My Drive/CSCI 470 Project/Epic/Data/Cleaned_UTF8/merged_Stephen_King_train.txt\n","Test Path:\t /content/drive/My Drive/CSCI 470 Project/Epic/Data/Cleaned_UTF8/merged_Stephen_King_test.txt\n","Output Directory Path:\t /content/drive/My Drive/CSCI 470 Project/Epic/Models/gpt2_small_Stephen_King\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='101' max='19284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  101/19284 02:58 < 9:36:02, 0.56 it/s, Epoch 0.02/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='234' max='16069' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  234/16069 04:01 < 4:33:08, 0.97 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Wks-Hgf9icH5"},"source":["\n","from transformers import pipeline\n","\n","generator = pipeline('text-generation',model=output_path+\"/checkpoint-000\", tokenizer=tokenizer ,config={'max_length':800})\n","\n","result = generator('Input Prompt')[0]['generated_text']"],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Hyperparameter_Tuning.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"4IwFotv8VirK"},"source":["import tensorflow as tf\n","import torch\n","from transformers import GPT2Model, GPT2Config\n","import ipywidgets\n","from IPython import display"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWpiAEBsVirS","outputId":"6cdbc5f0-4de6-47a9-fee8-93ca9b01ff38"},"source":["# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n","# evaluate the pre-trained model without finetuning.\n","# CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/presidential_speeches/checkpoint-1500'\n","CHECKPOINT_PATH = \"gpt2-medium\"\n","\n","# Set this to the list of text files you want to evaluate the perplexity of.\n","DATA_PATHS = [\"/content/presidential_speeches_valid.txt\",\n","              \"/content/presidential_speeches_test.txt\"]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","print(\"Running on device: \", device)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running on device:  cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pOF2yk2sVirX","outputId":"29c8dd24-2494-44e3-a74d-3ad2265b6f5d"},"source":["# Initializing a GPT2 configuration\n","configuration = GPT2Config()\n","# Initializing a model from the configuration\n","model = GPT2Model(configuration)\n","# Accessing the model configuration\n","configuration = model.config\n","print(configuration)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPT2Config {\n","  \"activation_function\": \"gelu_new\",\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"resid_pdrop\": 0.1,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"vocab_size\": 50257\n","}\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5ww17tp8Viri"},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n","# Download model and configuration from S3 and cache.\n","model = AutoModelWithLMHead.from_pretrained('gpt2-medium', pad_token_id=tokenizer.eos_token_id)\n","\n","\n","input_context = 'My cute dog'\n","\n","# These will be BlackList.txt words\n","bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\n","\n","input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpYaVMzdViro","outputId":"476340da-49a1-43e4-9dd5-04a6d9b5008a"},"source":["# Normal Output\n","outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\n","print(\"Normal Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Normal Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog\n","\n","He comes around the corner\n","\n","I wonder what he plans to do\n","\n","I wonder what he has planned\n","\n","What happened here today\n","\n","What happened here today\n","\n","That day she went out\n","\n","Yeah, that day she went out\n","\n","That day she went out\n","\n","That day she went out\n","\n","I think now I'll forget\n","\n","that day she went out\n","\n","that day she went out\n","\n","All we gotta do is work together\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fGO-snNSVirt","outputId":"10d8ed90-4488-45bd-fcf5-67e6736bd978"},"source":["# activate beam search and early_stopping\n","beam_output = model.generate(\n","    input_ids, \n","    max_length=100, \n","    num_beams=5, \n","    early_stopping=True\n",")\n","\n","print(\"Beam Search Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Beam Search Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much! I love you so much!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kn7XdAhrVirx","outputId":"1f210941-c588-4256-83bd-89721415aa79"},"source":["# set no_repeat_ngram_size to 2\n","beam_output = model.generate(\n","    input_ids, \n","    max_length=100, \n","    num_beams=6, \n","    no_repeat_ngram_size=6, \n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog! I love you so much!\"\n","\n","\"I love you too!\"\n","\n","\"I'm so happy for you!\"\n","\n","\"I can't wait to see you again!\"\n","\n","\"You're so cute!\"\n","\n","\"I miss you so much.\"\n","\n","\"I want to hug you.\"\n","\n","\"You're my best friend!\"\n","\n","\"I want you to be my best friend forever!\"\n","\n","\"I'll always love you!\"\n","\n","\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3UZ6p3quVir1","outputId":"cd429252-17e2-4a36-c019-01611d22ed9d"},"source":["# set return_num_sequences > 1\n","beam_outputs = model.generate(\n","    input_ids, \n","    max_length=100, \n","    num_beams=5, \n","    no_repeat_ngram_size=2, \n","    num_return_sequences=5, \n","    early_stopping=True\n",")\n","\n","# now we have 3 output sequences\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: My cute dog! I love you so much!\"\n","\n","\"I'm so happy to see you again. I'm glad you're okay. It's been a long time since I've seen you, but I know you'll be fine. You've been through a lot, haven't you? I can't imagine what it must have been like for you to be separated from your family and friends for so long. We're so lucky to have you with us. Thank you for everything.\"\n","\n","1: My cute dog! I love you so much!\"\n","\n","\"I'm so happy to see you again. I'm glad you're okay. It's been a long time since I've seen you, but I know you'll be fine. You've been through a lot, haven't you? I can't imagine what it must have been like for you to be separated from your family and friends for so long. We're so lucky to have you with us. Thank you for everything you've\n","2: My cute dog! I love you so much!\"\n","\n","\"I'm so happy to see you again. I'm glad you're okay. It's been a long time since I've seen you, but I know you'll be fine. You've been through a lot, haven't you? I can't imagine what it must have been like for you to be separated from your family and friends for so long. We're so lucky to have you with us. Thank you for everything you do\n","3: My cute dog! I love you so much!\"\n","\n","\"I'm so happy to see you again. I'm glad you're okay. It's been a long time since I've seen you, but I know you'll be fine. You've been through a lot, haven't you? I can't imagine what it must have been like for you to be separated from your family and friends for so long. We're so lucky to have you with us, and we'll miss you.\"\n","4: My cute dog! I love you so much!\"\n","\n","\"I'm so happy to see you again. I'm glad you're okay. It's been a long time since I've seen you, but I know you'll be fine. You've been through a lot, haven't you? I can't imagine what it must have been like for you to be separated from your family and friends for so long. We're so lucky to have you with us. Thank you for everything you did\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mgGZg_CoVir6","outputId":"d3b137ea-2f54-411e-e70e-8ef1984d8be1"},"source":["# activate sampling and deactivate top_k by setting top_k sampling to 0\n","sample_output = model.generate(\n","    input_ids, \n","    do_sample=True, \n","    max_length=100, \n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog ends up walking all the street corners before they complete her her routine!\" Kit found that out all too late. \"As soon as I heard her stop she did something funny with my puppet, I snatched her balloons before her and the kids followed!\"\n","\n","JUST AS VETEANS GIVEN DONATION CUSTOMS WERE DECIDED BROADCASTED, THE PRANK GOT MOVED AHEAD BY FRIENDS THAT IN CHANGES AT WORK, AN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RodtkJi2Vir-","outputId":"3fd75fdb-a423-41c4-ff3f-9386191264a4"},"source":["# use temperature to decrease the sensitivity to low probability candidates\n","sample_output = model.generate(\n","    input_ids, \n","    do_sample=True, \n","    max_length=100, \n","    top_k=0, \n","    temperature=0.7\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog was so happy to see me come through the door, because I was the one who walked him through the door. He loved it too!\"\n","\n","Paula also shared that her husband was very excited for his new baby girl. And she told us that her husband would be \"happy to know we are getting a kid.\"\n","\n","She also shared that the baby is also a \"bright smile\" and loves to play.\n","\n","You can watch the full interview on their Facebook page\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PJUV7xYTVisC","outputId":"2799f7bc-bfb9-4f76-ebfb-2135dd780aa4"},"source":["# set top_k to 50\n","sample_output = model.generate(\n","    input_ids, \n","    do_sample=True, \n","    max_length=100, \n","    top_k=50\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog. I'll be your mommy.\" She kissed his mouth briefly and gave him a tiny nod. \"You're adorable. Thanks.\" She looked out back at the river and the stars. \"When can we take the dog back?\" \"Maybe later. I won't tell anyone.\" She nodded to the dog. \"I know how to take care of him now.\" \"So…when can we meet the people that are going to be taking us when we do get home?\" Anna\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l6CN51BaVisF","outputId":"0f047e41-e953-4245-e476-46dfa3e229bb"},"source":["# deactivate top_k sampling and sample only from 92% most likely words\n","sample_output = model.generate(\n","    input_ids, \n","    do_sample=True, \n","    max_length=100, \n","    top_p=0.92, \n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","My cute dog, us's less than five months old, also came to the school, coming in very hungry. I tried to feed them at the checkout stand, but the ticket got away.\"\n","\n","When the cameras caught the incident they were stopped by school staff and paid more than $300 in legal fees. Teachers will have to decide whether to recommend owners have them registered as commercial crop bed dog residents.\n","\n","But those school patrols have sometimes caused confused reactions among some residents who believe that the\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1aMA_0UJVisJ","outputId":"4a8dfb1c-d037-4bd2-f46e-27822e5a9bdb"},"source":["# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","sample_outputs = model.generate(\n","    input_ids,\n","    do_sample=True, \n","    max_length=100, \n","    top_k=50, \n","    top_p=0.95, \n","    num_return_sequences=3\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, sample_output in enumerate(sample_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: My cute dog and I ran to her house when we were little. We have lived at our house for 2 years. My daughter bought her a new dress for Christmas and we both love it. She doesn't know she was born bald, but she doesn't care about it. The whole situation was a great blessing to us and it was beautiful!\n","\n","This article also appeared on our sister blog: I Dream Of My Family. You can follow our blog @italongdaniel for everything family\n","1: My cute dog loves me. She gets her treats every single day! She's our dog, and I'm the only one who sees her all the time. So it was hard for me to take all of the negative pressure off my shoulders when my dog got attacked last night.\"\n","\n","And that's where the similarities end. When you've tried to take a life and take away a person's right to life, it makes it harder to look upon those things as friends.\n","\n","On the\n","2: My cute dog was so upset and he yelled at me! He came to try and find me!\" she said.\n","\n","Cecilia said she did not feel safe for about 2 hours until police came to her side. \"I was screaming but I couldn't see because of the sun light,\" she said. \"Then the police said there's nothing they could do. I kept saying my dog has to go but I don't believe that because I live right on top of them.\"\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"axx50VY2VisN"},"source":["Worrying but funny output:\n","\n","\n","\"My cute dog was so upset and he yelled at me! He came to try and find me!\" she said.\n","Cecilia said she did not feel safe for about 2 hours until police came to her side. \"I was screaming but I couldn't see because of the sun light,\" she said. \"Then the police said there's nothing they could do. I kept saying my dog has to go but I don't believe that because I live right on top of them.\"\n","\n","\n","* min_length: can be used to force the model to not produce an EOS token (= not finish the sentence) before min_length is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n","\n","* repetition_penalty: can be used to penalize words that were already generated or belong to the context. It was first introduced by Kesker et al. (2019) and is also used in the training objective in Welleck et al. (2019). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, e.g. see this discussion on Github.\n","\n","* attention_mask: can be used to mask padded tokens\n","\n","* pad_token_id, bos_token_id, eos_token_id: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n","\n"]}]}
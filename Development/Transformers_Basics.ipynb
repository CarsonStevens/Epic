{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"Transformers_Basics.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"7TlzdehxkM-R"},"source":["<h1><a href=\"https://huggingface.co/transformers/model_doc/gpt2.html\">HuggingFace OpenAI GPT2</a> <a href=\"https://huggingface.co/exbert/?model=gpt2&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=11&heads=..&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=false\">Transformer Visualizer</a></h1>\n","<h4><a href=\"https://huggingface.co/transformers/main_classes/processors.html\">List of Data Processors</a></h4>\n","<h4><a href=\"https://huggingface.co/transformers/pretrained_models.html\">List of Pretrained Models</a></h4>\n","<h4><a href=\"https://huggingface.co/transformers/main_classes/tokenizer.html\">List of Tokenizers</a></h4>\n","<h4><a href=\"https://huggingface.co/transformers/main_classes/pipelines.html\">List of Pipelines</a></h4>\n","<h4><a href=\"https://huggingface.co/transformers/main_classes/optimizer_schedules.html\">List of Optimizers</a></h4>\n","\n","<h3>Installation</h3>\n","\n",">pip install transformers\\[torch]\n","\n",">pip install transformers\\[tf-cpu]\n","\n","If you don‚Äôt have any specific environment variable set, the cache directory will be at ~/.cache/torch/transformers/.\n","\n","\n","<h2>Text Generation</h2>\n","In text generation (a.k.a open-ended text generation) the goal is to create a coherent portion of text that is a continuation from the given context. The following example shows how GPT-2 can be used in pipelines to generate text. As a default all models apply Top-K sampling when used in pipelines, as configured in their respective configurations (see gpt-2 config for example).\n","\n","<pre><code>\n","from transformers import pipeline\n","\n","text_generator = pipeline(\"text-generation\")\n","\n","print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))\n","\n","\n","\n","from transformers import GPT2Tokenizer, <em>TFGPT2Model</em>\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = <em>TFGPT2Model.from_pretrained('gpt2')</em>\n","\n","text = \"Replace me by any text you'd like.\"\n","encoded_input = tokenizer(text, return_tensors='tf')\n","output = model(encoded_input)\n","\n","</pre></code>\n","\n",">>[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]\n","\n","\n","Here, the model generates a random text with a total maximal length of 50 tokens from context ‚ÄúAs far as I am concerned, I will‚Äù. The default arguments of PreTrainedModel.generate() can be directly overriden in the pipeline, as is shown above for the argument max_length.\n","\n","<h2>Text Summarization</h2>\n","Summarization is the task of summarizing a document or an article into a shorter text.\n","\n","An example of a summarization dataset is the CNN / Daily Mail dataset, which consists of long news articles and was created for the task of summarization. If you would like to fine-tune a model on a summarization task, various approaches are described in this document.\n","\n","Here is an example of using the pipelines to do summarization. It leverages a Bart model that was fine-tuned on the CNN / Daily Mail data set.\n","<pre><code>\n","from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\")\n","ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n","<em>...</em> \n","<em>...</em> \n","If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n","</pre></code>\n","\n","Because the summarization pipeline depends on the PretrainedModel.generate() method, we can override the default arguments of PretrainedModel.generate() directly in the pipeline for max_length and min_length as shown below. This outputs the following summary:\n","\n","<pre><code>\n","    print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False\n","</pre></code>\n",">>[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n","\n","<h4> Using Model and Tokenizer Example</h4>\n","Here is an example of doing summarization using a model and a tokenizer. The process is the following:\n","\n","1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder model, such as Bart or T5.\n","\n","2. Define the article that should be summarized.\n","\n","3. Add the T5 specific prefix ‚Äúsummarize: ‚Äú.\n","\n","4. Use the PretrainedModel.generate() method to generate the summary.\n","\n","<pre><code>\n","from transformers import AutoModelWithLMHead, AutoTokenizer\n","\n","model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n","\n","\\#T5 uses a max_length of 512 so we cut the article to 512 tokens.\n","\n","inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n","outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n","</pre></code>\n","\n","<h2>Fine Tuning</h2>\n","<h3> Fine-tuning with <a href=\"https://huggingface.co/transformers/main_classes/trainer.html\">Trainer</a></h3>\n","<pre>\n","<code>\n","from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n","\n","training_args = TFTrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n",")\n","\n","with training_args.strategy.scope():\n","    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n","\n","trainer = TFTrainer(\n","    model=model,                         # the instantiated ü§ó Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","</code>\n","</pre>\n","\n","<h3>Fine-tuning with Tensorflow</h3>\n","<pre>\n","<code>\n","from torch.utils.data import DataLoader\n","from transformers import DistilBertForSequenceClassification, AdamW\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","model.to(device)\n","model.train()\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","optim = AdamW(model.parameters(), lr=5e-5)\n","\n","for epoch in range(3):\n","    for batch in train_loader:\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()\n","\n","model.eval()\n","</code>\n","</pre>"]},{"cell_type":"code","metadata":{"id":"duNzx5TSkM-R"},"source":["import tensorflow as tf\n","from transformers import GPT2Model, GPT2Config\n","import ipywidgets\n","from IPython import display\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ki1uqbJYkM-R"},"source":["\n","# Initializing a GPT2 configuration\n","configuration = GPT2Config()\n","# Initializing a model from the configuration\n","model = GPT2Model(configuration)\n","# Accessing the model configuration\n","configuration = model.config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vm1GQp6pkM-R"},"source":["<h1>Tokenization</h1>\n","\n","‚ÜíEvery transformer model has a similar token definition API\n","\n","‚ÜíHere I am using a tokenizer from a Pretrained model.\n","\n","‚ÜíHere,\n","\n","* add_special_tokens: Is used to add special character like <cls>, <sep>,<unk>, etc w.r.t Pretrained model in use. It should be always kept True\n","* max_length: Max length of any sentence to tokenize, its a hyperparameter. (originally BERT has 512 max length)\n","pad_to_max_length: perform padding operation.\n"]},{"cell_type":"code","metadata":{"id":"SiBPsiUakM-R","outputId":"2ea1f1f0-b79e-4719-9554-8e89f44e2258","colab":{"referenced_widgets":["267ff9da84f7470e9079ed66dada1337","2a6f5c72185c4744817e465b3a8636c0"]}},"source":["from transformers import DistilBertTokenizer, RobertaTokenizer \n","distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n","roberta = 'roberta-base'\n","\n","# Defining DistilBERT tokonizer\n","tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n","                                                max_length=128, pad_to_max_length=True)\n","# Defining RoBERTa tokinizer\n","tokenizer = RobertaTokenizer.from_pretrained(roberta, do_lower_case=True, add_special_tokens=True,\n","                                                max_length=128, pad_to_max_length=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"267ff9da84f7470e9079ed66dada1337","version_major":2,"version_minor":0},"text/plain":["HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898823.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a6f5c72185c4744817e465b3a8636c0","version_major":2,"version_minor":0},"text/plain":["HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a0s26r6rkM-S"},"source":["<h2>Tokenize Document</h2>\n","\n","‚ÜíAny transformer model generally needs three input:\n","\n","* input ids: word id associated with their vocabulary\n","\n","* attention mask: Which id must be paid attention to; 1=pay attention. In simple terms, it tells the model which are original words and which are padded words or special tokens\n","\n","* token type id: It's associated with model consuming multiply sentence like Question-Answer model. It tells model about the sequence of the sentences.\n","\n","‚ÜíThough it is not compulsory to provide all these three ids and only input ids will also do, but attention mask help model to focus on only valid words. So at least for classification task both this should be provided.\n"]},{"cell_type":"code","metadata":{"id":"lMnu69gHkM-S"},"source":["def tokenize(sentences, tokenizer):\n","    input_ids, input_masks, input_segments = [],[],[]\n","    for sentence in tqdm(sentences):\n","        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n","                                             return_attention_mask=True, return_token_type_ids=True)\n","        input_ids.append(inputs['input_ids'])\n","        input_masks.append(inputs['attention_mask'])\n","        input_segments.append(inputs['token_type_ids'])        \n","        \n","    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZbdI3wIFkM-S"},"source":["<h1>Training and Fine-Tuning</h1>\n","\n","<h2>Use Pretrained Model directly as Classifier</h2>\n","\n","Hugging Face‚Äôs transformers library provide some models with sequence classification ability. These model have two heads, one is a pre-trained model architecture as the base & a classifier as the top head.\n","\n","Tokenizer definition ‚ÜíTokenization of Documents ‚ÜíModel Definition"]},{"cell_type":"code","metadata":{"id":"PZFkVFcrkM-S"},"source":["from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n","import tensorflow as tf\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n","inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1)) # Batch size 1\n","outputs = model(inputs)\n","loss, logits = outputs[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHN9AMOUkM-T","outputId":"4e94e5f9-efeb-4500-92ad-6e59f89c27d3"},"source":["from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n","import tensorflow as tf\n","\n","\n","distil_bert = 'distilbert-base-uncased'\n","\n","config = DistilBertConfig(num_labels=6)\n","config.output_hidden_states = False\n","transformer_model = TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config)\n","\n","input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n","input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n","X = transformer_model(input_ids, input_masks_ids)\n","model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_99']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-DQG6LE5kM-T","outputId":"ef5ff2bd-0f56-4127-8df7-26066dde5e91"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_token (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","masked_token (InputLayer)       [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","tf_distil_bert_for_sequence_cla ((None, 6),)         66958086    input_token[0][0]                \n","                                                                 masked_token[0][0]               \n","==================================================================================================\n","Total params: 66,958,086\n","Trainable params: 66,958,086\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IuZzhgYmkM-T"},"source":["‚ÜíNote: Models which are SequenceClassification are only applicable here.\n","\n","‚ÜíDefining the proper config is crucial here. As you can see on line 6, I am defining the config. ‚Äònum_labels‚Äô is the number of classes to use when the model is a classification model. It also supports a variety of configs so go ahead & see their docs.\n","\n","‚Üí Some key things to note here are:\n","\n","* Here only weights of the pre-trained model can be updated, but updating them is not a good idea as it will defeat the purpose of transfer learning. So, actually there is nothing here to update. This is the reason I least prefer this.\n","* It is also the least customizable.\n","* A hack you can try is using num_labels with much higher no and finally adding a dense layer at the end which can be trained."]},{"cell_type":"code","metadata":{"id":"FgfEYYeUkM-T"},"source":["# # Hack\n","# config = DistilBertConfig(num_labels=64)\n","# config.output_hidden_states = False\n","# transformer_model=TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config) \n","# input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n","# input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n","# X = transformer_model(input_ids, input_masks_ids)[0]\n","# X = tf.keras.layers.Dropout(0.2)(X)\n","# X = tf.keras.layers.Dense(6, activation='softmax')\n","# model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n","# for layer in model.layer[:2]:\n","#     layer.trainable = False\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Q6YEhtekM-T"},"source":["<h2>Transformer model to extract embedding and use it as input to another classifier</h2>\n","\n","This approach needs two level or two separate models. We use any transformer model to extract word embedding & then use this word embedding as input to any classifier (eg Logistic classifier, Random forest, Neural nets, etc).\n","I would suggest you read this <a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\">article</a> by Jay Alammar which discusses this approach with great detail and clarity.\n","As this blog is all about neural nets, let me give you an example of this approach with NN.\n","\n","‚ÜíLine 12 is key here. We are only interested in <cls> or classification token of the model which can be extracted using the slice operation. Now we have 2D data and build the network as one desired.\n"]},{"cell_type":"code","metadata":{"id":"BIGaY9zfkM-T","outputId":"338a59b5-2fc8-4db4-ede9-687632d2cd24"},"source":["from transformers import TFDistilBertModel\n","distil_bert = 'distilbert-base-uncased'\n","\n","config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n","config.output_hidden_states = False\n","transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n","\n","input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n","input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n","\n","embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n","cls_token = embedding_layer[:,0,:]\n","X = tf.keras.layers.BatchNormalization()(cls_token)\n","X = tf.keras.layers.Dense(192, activation='relu')(X)\n","X = tf.keras.layers.Dropout(0.2)(X)\n","X = tf.keras.layers.Dense(6, activation='softmax')(X)\n","model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n","\n","for layer in model.layers[:3]:\n","  layer.trainable = False"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n","- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"b4lJlgrakM-T","outputId":"da248924-94ce-409a-cd94-bbf32aceffb7"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_6\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_token (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","masked_token (InputLayer)       [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","tf_distil_bert_model_1 (TFDisti ((None, 128, 768),)  66362880    input_token[0][0]                \n","                                                                 masked_token[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           tf_distil_bert_model_1[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 768)          3072        tf_op_layer_strided_slice_1[0][0]\n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 192)          147648      batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","dropout_160 (Dropout)           (None, 192)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 6)            1158        dropout_160[0][0]                \n","==================================================================================================\n","Total params: 66,514,758\n","Trainable params: 150,342\n","Non-trainable params: 66,364,416\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kedmPwt_kM-T"},"source":["<h3>Just extract word embedding</h3>"]},{"cell_type":"code","metadata":{"id":"E1g7FA2gkM-T"},"source":["# import numpy as np\n","# from transformers import AutoTokenizer, pipeline, TFDistilBertModel\n","# model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n","# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","# pipe = pipeline('feature-extraction', model=model, \n","#                 tokenizer=tokenizer)\n","# features = pipe('any text data or list of text data',\n","#                 pad_to_max_length=True)\n","# features = np.squeeze(features)\n","# features = features[:,0,:]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jIKq3ExNkM-T"},"source":["<h2>Fine-Tuning a Pretrained transformer model</h2>\n","\n","‚ÜíLook at line 17 as 3D data is generated earlier embedding layer, we can use LSTM to extract great details.\n","\n","‚ÜíNext thing is to transform the 3D data into 2D so that we can use a FC layer. You can use any Pooling layer to perform this.\n","\n","‚Üí Also, note on line 18 & 19. We should always freeze the pre-trained weights of transformer model & never update them and update only remaining weights.\n","Some extras\n","\n","‚ÜíEvery approach has two things in common:\n","config.output_hidden_states=False; as we are training & not interested in output state.\n","X = transformer_model(‚Ä¶)\\[0]; this is inline in config.output_hidden_states as we want only the top head.\n","\n","‚Üíconfig is a dictionary. So to see all available configuration, just simply print it.\n","\n","‚ÜíChoose base model carefully as TF 2.0 support is new, so there might be bugs."]},{"cell_type":"code","metadata":{"id":"3_9_53A6kM-T","outputId":"1d32ae12-58bd-41fd-ac2f-fce802af1108"},"source":["distil_bert = 'distilbert-base-uncased'\n","\n","config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n","config.output_hidden_states = False\n","transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n","\n","input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n","input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n","\n","embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n","X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n","X = tf.keras.layers.GlobalMaxPool1D()(X)\n","X = tf.keras.layers.Dense(50, activation='relu')(X)\n","X = tf.keras.layers.Dropout(0.2)(X)\n","X = tf.keras.layers.Dense(6, activation='sigmoid')(X)\n","model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n","\n","for layer in model.layers[:3]:\n","  layer.trainable = False"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n","- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"G1oKVVSTkM-T","outputId":"1a6568ce-0af8-49d4-f952-86a4b90e0905"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_8\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_token (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","masked_token (InputLayer)       [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","tf_distil_bert_model_3 (TFDisti ((None, 128, 768),)  66362880    input_token[0][0]                \n","                                                                 masked_token[0][0]               \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 128, 100)     327600      tf_distil_bert_model_3[0][0]     \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 100)          0           bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 50)           5050        global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","dropout_199 (Dropout)           (None, 50)           0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 6)            306         dropout_199[0][0]                \n","==================================================================================================\n","Total params: 66,695,836\n","Trainable params: 332,956\n","Non-trainable params: 66,362,880\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s5lZ8w14kM-U"},"source":["<h3>For Data Output Cleaning</h3>\n","You can actually use bad_words_id parameter with a line break, which will prevent generate function from giving you results, which contain \"\\n\". (though you'd probably have to add every id from your vocab, which has line breaks in it, since I do think there tends to be more than one \"breaking\" sequence out there...)"]},{"cell_type":"code","metadata":{"id":"UE1UaRDukM-U"},"source":[""],"execution_count":null,"outputs":[]}]}
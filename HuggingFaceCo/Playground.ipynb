{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"https://huggingface.co/transformers/model_doc/gpt2.html\">HuggingFace OpenAI GPT2</a> <a href=\"https://huggingface.co/exbert/?model=gpt2&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=11&heads=..&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=false\">Transformer Visualizer</a></h1>\n",
    "<h4><a href=\"https://huggingface.co/transformers/main_classes/processors.html\">List of Data Processors</a></h4>\n",
    "<h4><a href=\"https://huggingface.co/transformers/pretrained_models.html\">List of Pretrained Models</a></h4>\n",
    "<h4><a href=\"https://huggingface.co/transformers/main_classes/tokenizer.html\">List of Tokenizers</a></h4>\n",
    "<h4><a href=\"https://huggingface.co/transformers/main_classes/pipelines.html\">List of Pipelines</a></h4>\n",
    "<h4><a href=\"https://huggingface.co/transformers/main_classes/optimizer_schedules.html\">List of Optimizers</a></h4>\n",
    "\n",
    "<h3>Installation</h3>\n",
    "\n",
    ">pip install transformers\\[torch]\n",
    "\n",
    ">pip install transformers\\[tf-cpu]\n",
    "\n",
    "If you don‚Äôt have any specific environment variable set, the cache directory will be at ~/.cache/torch/transformers/.\n",
    "\n",
    "\n",
    "<h2>Text Generation</h2>\n",
    "In text generation (a.k.a open-ended text generation) the goal is to create a coherent portion of text that is a continuation from the given context. The following example shows how GPT-2 can be used in pipelines to generate text. As a default all models apply Top-K sampling when used in pipelines, as configured in their respective configurations (see gpt-2 config for example).\n",
    "\n",
    "<pre><code>\n",
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, <em>TFGPT2Model</em>\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = <em>TFGPT2Model.from_pretrained('gpt2')</em>\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "</pre></code>\n",
    "\n",
    ">>[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]\n",
    "\n",
    "\n",
    "Here, the model generates a random text with a total maximal length of 50 tokens from context ‚ÄúAs far as I am concerned, I will‚Äù. The default arguments of PreTrainedModel.generate() can be directly overriden in the pipeline, as is shown above for the argument max_length.\n",
    "\n",
    "<h2>Text Summarization</h2>\n",
    "Summarization is the task of summarizing a document or an article into a shorter text.\n",
    "\n",
    "An example of a summarization dataset is the CNN / Daily Mail dataset, which consists of long news articles and was created for the task of summarization. If you would like to fine-tune a model on a summarization task, various approaches are described in this document.\n",
    "\n",
    "Here is an example of using the pipelines to do summarization. It leverages a Bart model that was fine-tuned on the CNN / Daily Mail data set.\n",
    "<pre><code>\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "<em>...</em> \n",
    "<em>...</em> \n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "</pre></code>\n",
    "\n",
    "Because the summarization pipeline depends on the PretrainedModel.generate() method, we can override the default arguments of PretrainedModel.generate() directly in the pipeline for max_length and min_length as shown below. This outputs the following summary:\n",
    "\n",
    "<pre><code>\n",
    "    print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False\n",
    "</pre></code>\n",
    ">>[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n",
    "\n",
    "<h4> Using Model and Tokenizer Example</h4>\n",
    "Here is an example of doing summarization using a model and a tokenizer. The process is the following:\n",
    "\n",
    "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder model, such as Bart or T5.\n",
    "\n",
    "2. Define the article that should be summarized.\n",
    "\n",
    "3. Add the T5 specific prefix ‚Äúsummarize: ‚Äú.\n",
    "\n",
    "4. Use the PretrainedModel.generate() method to generate the summary.\n",
    "\n",
    "<pre><code>\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "\\#T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "\n",
    "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n",
    "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "</pre></code>\n",
    "\n",
    "<h2>Fine Tuning</h2>\n",
    "<h3> Fine-tuning with <a href=\"https://huggingface.co/transformers/main_classes/trainer.html\">Trainer</a></h3>\n",
    "<pre>\n",
    "<code>\n",
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "<h3>Fine-tuning with Tensorflow</h3>\n",
    "<pre>\n",
    "<code>\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', return_dict=True)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

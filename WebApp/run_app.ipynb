{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atX7tHyJMrcI","outputId":"3f1665be-e5cf-4aff-961c-a094c1eee0dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/19Z9fYLUW5aaRDu2a-rq0Ev2uiUl6J2ln/CSCI 470 Project/Epic\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 12.4MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 48.7MB/s \n","\u001b[?25hCollecting flask-ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 44.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 47.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n","Requirement already satisfied: dataclasses; python_version \u003c \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: Flask\u003e=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers) (0.17.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (2020.12.5)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-\u003etransformers) (2.4.7)\n","Requirement already satisfied: itsdangerous\u003e=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask\u003e=0.8-\u003eflask-ngrok) (1.1.0)\n","Requirement already satisfied: Werkzeug\u003e=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask\u003e=0.8-\u003eflask-ngrok) (1.0.1)\n","Requirement already satisfied: Jinja2\u003e=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask\u003e=0.8-\u003eflask-ngrok) (2.11.2)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2\u003e=2.10.1-\u003eFlask\u003e=0.8-\u003eflask-ngrok) (1.1.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=9787183f3487c8477df2e1f92bb93bcea5cdef1b30aaf224f0a4e8fc6ae2db25\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","from IPython import display\n","%cd /content/drive/MyDrive/CSCI 470 Project/Epic\n","!pip install transformers sentencepiece flask-ngrok\n","display.clear_output()\n","\n","import warnings\n","import pprint\n","warnings.filterwarnings(\"ignore\")\n","\n","from flask_ngrok import run_with_ngrok\n","from flask import Flask, request, url_for, redirect, request\n","from flask import render_template, render_template_string\n","from pathlib import Path\n","import logging\n","import json\n","import time\n","import re\n","import os\n","from pprint import pprint\n","from tqdm import tqdm\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelWithLMHead\n","from transformers import AutoModelForSequenceClassification\n","from transformers import pipeline\n","\n","\n","display.clear_output()\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","print(\"\\nRunning on device: \", str(device).upper())\n","\n","if gpu_info.find('failed') \u003e= 0 and ram_gb \u003c 30:\n","  print('\\nSelect the Runtime \u003e \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","  print('\\nTo enable a high-RAM runtime, select the Runtime \u003e \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  try:\n","      if gpu_info.find('failed') \u003c 0:\n","         print(gpu_info)\n","  except:\n","    display.clear_output()\n","  finally:\n","    print('\\nYour runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","    print('You are using a high-RAM runtime!')\n","\n","\n","literary_tokens = [\"characterization\", \"character\", \"setting\", \n","                   \"exposition\", \"climax\", \"resolution\", \"plot\", \n","                   \"context\", \"action\", \"weapon\", \n","                   \"danger\", \"death\", \"suspense\", \n","                   \"emotion\", \"surprise\", \"problem\", \"conflict\", \n","                   \"perspective\", \"transition\", \"relief\", \"metaphor\", \n","                   \"flashback\"]\n","\n","subgenre_tokens = ['Vampire', 'Ghost', 'Horror', 'Comedic Horror', 'Murder', \n","                   'Werewolf', 'Apocalypse','Haunted House', 'Witch', 'Hell', \n","                   'Alien', 'Gore', 'Monster']\n","\n","\n","author_tokens = {'Clive Barker' : '[CLIVE BARKER]', 'J. K. Rowling' : '[J.K. ROWLING]', 'Stephen King' : '[STEPHEN KING]', 'Théophile Gautier' : '[THEOPHILE GAUTIER]', \n","            'James H. Hyslop' : '[JAMES H HYSLOP]', 'Lord Edward Bulwer-Lytton' : '[LORD EDWARD BULWER-LYTTON]', 'A. T. Quiller-Couch' : '[A. T. QUILLER-COUCH]', \n","            'Mrs. Margaret Oliphant' : '[MRS. MARGARET OLIPHANT]', 'Ernest Theodor Amadeus Hoffmann' : '[ERNEST THEODOR AMADEUS HOFFMAN]', 'Erckmann-Chatrian' : '[ERCKMANN-CHATRAIN]', \n","            'Fiona Macleod' : '[FIONA MACLEOD]', 'Amelia B. Edwards' : '[AMELIA B. EDWARDS]', 'H. B. Marryatt' : '[H. B. MARRYATT]', 'Thomas Hardy' : '[THOMAS HARDY]', \n","            'Montague Rhodes James' : '[MONTAGUE RHODES JAMES]', 'Fitz-James O\\'Brien' : '[FITZ-JAMES O\\'BRIEN', 'James Stephen' : '[JAMES STEPHEN]', 'Alfred Lord Tennyson' : '[ALFRED LORD TENNYSON]',\n","            'Amelia Edwards' : '[AMELIA EDWARDS]', 'Edward Bulwer-Lytton' : '[EDWARD BULWER-LYTTON]', 'Erckmann Chatrian' : '[ERCKMANN CHATRIAN]', 'Latifa al-Zayya' : '[LATIFA AL-ZAYYA]',\n","            'M. R. James' : '[M. R. JAMES]', 'Paul Brandis' : '[PAUL BRANDIS]', 'Brain Evenson' : '[BRAIN EVENSON]', 'Elliott O\\'Donnell' : '[ELLIOTT O\\'DONNELL]', \n","            'Joseph, Sheridan Le Fanu' : '[JOSEPH, SHERIDAN LE FANU]', 'Edgar Allan Poe' : '[EDGAR ALLEN POE]', 'Bram Stoker' : '[BRAM STOKER]', 'Algernon Blackwood' :'[ALGERNON BLACKWOOD]',\n","            'Miles Klee' : '[MILES KLEE]', 'Nnedi Okorador' : '[NNEDI OKORADOR]', 'Sofia Samatar' : '[SOFIA SAMATAR]', 'Franz Kafka' : '[FRANZ KAFKA]', 'Laird Barron' : '[LAIRD BARRON]',\n","            'Nathan Ballingrud' : '[NATHAN BALLINGRUD]', 'Nellie Bly' : '[NELLIE BLY]', 'William Hop Hodgson' : '[WILLIAM HOP HODGSON]', 'Ambrose Bierce' : '[AMBROSE BIERCE]',\n","            'Kelly Link' : '[KELLY LINK]', 'Arthur Machen' : '[ARTHUR MACHEN]', 'George Sylvester Viereck' : '[GEORGE SYLVESTER VIERECK]', 'Robert Chambers' : '[ROBERT CHAMBERS]',\n","            'John Meade Falkner' : '[JOHN MEADE FALKNER]', 'Ann Radcliffe' : '[ANN RADCLIFFE]', 'Howard Lovecraft' : '[HOWARD LOVECRAFT]', 'Louis Stevenson' : '[LOUIS STEVENSON]',\n","            'Edith Birkhead' : '[EDITH BIRKHEAD]', 'Jeff Vandermeer' : '[JEFF VANDERMEER]', 'Henry James' : '[HENRY JAMES]', 'John William Polidori' : '[JOHN WILLIAM POLIDORI]',\n","            'Bob Holland' : '[BOB HOLLAND]', 'Oliver Onions' : '[OLIVER ONIONS]'}\n","\n","def generate_checkbox(val, classes):\n","    return f'''\n","    \u003cdiv class=\"checkbox-container {classes}\"\u003e\n","        \u003cspan class=\"input-title\"\u003e{val}\u003c/span\u003e\n","        \u003clabel class=\"checkbox-label\"\u003e\n","            \u003cinput type=\"checkbox\" value=\"{val}\"\u003e\n","            \u003cspan class=\"checkbox-custom rectangular\"\u003e\u003c/span\u003e\n","        \u003c/label\u003e\n","    \u003c/div\u003e\n","    '''\n","\n","def generate_inputs():\n","    author_inputs = f''''''\n","    for author in author_tokens:\n","        author_inputs += generate_checkbox(author, \"author\")\n","    \n","\n","    genre_inputs = f''''''\n","    for genre in subgenre_tokens:\n","        genre_inputs += generate_checkbox(genre, \"genre\")\n","        \n","    return author_inputs, genre_inputs\n","\n","def get_blacklist_inputs(blacklist_path=\"Data/Cleaned_Black_List.txt\"):\n","    blacklist_words = f''''''\n","    with open(blacklist_path, \"r\") as reader:\n","        lines = reader.readlines()\n","        for line in lines:\n","            blacklist_words += f'''\u003cli class=\"blacklist\" data-word=\"{line.lstrip().rstrip()}\"\u003e\u003ci class=\"fas fa-minus-circle delete\"\u003e\u003c/i\u003e\u003c/li\u003e\\n'''\n","    return blacklist_words\n","\n","def classify_input_tokens(input_text, threshold=0.9):\n","    tokens=[]\n","    #classify the lines according to literary tokens\n","    literary_generator = zeroshot_generator(input_text, literary_tokens, multi_class=True)\n","    for i, score in enumerate(literary_generator['scores']):\n","        if score \u003e threshold:\n","            tokens.append(\"[\" + literary_generator['labels'][i].upper() + \"]\")\n","        else: break\n","    return tokens\n","\n","\n","def get_bad_word_ids(bad_words=None, blacklist_path=\"Data/Cleaned_Black_List.txt\"):\n","    if not bad_words:\n","        bad_words = []\n","        with open(blacklist_path, \"r\") as reader:\n","            lines = reader.readlines()\n","            for word in lines:\n","                if (len(word) \u003e 0):\n","                    bad_words.append(word)\n","    bad_word_ids = [gpt2_tokenizer.encode(bad_word) for bad_word in bad_words]\n","    return bad_word_ids\n","\n","def remove_special_tokens(input):\n","    token_pattern = r\"[^[]*\\[([^]]*)\\]\"\n","    input = re.sub(token_pattern, \"\", input)\n","    return input\n","\n","def find_dialogue_locs(text):\n","    dialogue_locations = []\n","    dialogue_pattern = r'\"(?:(?:(?!(?\u003c!\\\\)\").)*)[.?!,]\"'\n","    for match in re.finditer(dialogue_pattern, text):\n","        s = match.start()\n","        e = match.end()\n","        dialogue_locations.append((s, e))\n","    return dialogue_locations\n","\n","def honorific_found(text, index):\n","    pat_obj = re.compile('(Mr)|(Mrs)|(Dr)|(Ms)|(Sr)|(Jr)|(Mt)', re.IGNORECASE)\n","    if pat_obj.search(text[index-4: index]):\n","        return True\n","    return False\n","\n","def dialogue_found(locs, index):\n","    for s, e in locs:\n","        if (s \u003c= index) and (index \u003c= e):\n","            return True\n","    return False\n","\n","def find_sentence_locs(text):\n","    #find and store locations of quotations within text \n","    dialogue_locations = find_dialogue_locs(text)\n","    punc_locations = []\n","    for match in re.finditer(\"[!.?]\", text):\n","        punc_i = match.end()\n","        if honorific_found(text, punc_i):\n","            continue      \n","        if dialogue_found(dialogue_locations, punc_i):\n","            continue\n","        punc_locations.append(punc_i)\n","    return punc_locations\n","\n","def generate_outputs(sequence, context_input, bad_words_ids, num_sequences=3, top_k=50, top_p=0.97, max_length=1024, temperature=0.8):\n","    input_ids = gpt2_tokenizer.encode(sequence, return_tensors='pt').to(device)  # encode input context\n","    sample_outputs = gpt2_model.generate(\n","        input_ids,\n","        do_sample=True, \n","        max_length=max_length, \n","        top_k=top_k, \n","        top_p=top_p,\n","        temperature=temperature,\n","        no_repeat_ngram_size=4,\n","        num_return_sequences=num_sequences, \n","        bad_words_ids=bad_words_ids,\n","        early_stopping=True\n","    )\n","    decoded_outputs = []\n","    for i, sample_output in enumerate(sample_outputs):\n","        output = remove_special_tokens(gpt2_tokenizer.decode(sample_output, skip_special_tokens=True))\n","        output = output.replace(context_input, \"\").strip()\n","        decoded_outputs.append(output)\n","\n","    return decoded_outputs\n","\n","def print_remove(outputs, width=80):\n","    for output in outputs:\n","        pprint(remove_special_tokens(output), width=width)\n","\n","def get_max_tokenizer(input, context):\n","    gpt2_tokens = gpt2_tokenizer.tokenize(context+input)\n","    contradiction_tokens = contradiction_tokenizer.tokenize(context+input)\n","    sentiment_tokens = sentiment_tokenizer.tokenize(context+input)\n","    max_tokenizer = gpt2_tokenizer\n","    max = len(gpt2_tokens)\n","    if len(contradiction_tokens) \u003e max:\n","        max_tokenizer = contradiction_tokenizer\n","        max = len(contradiction_tokens)\n","    if len(sentiment_tokens) \u003e max:\n","        max_tokenizer = sentiment_tokenizer\n","        max = len(sentiment_tokens)\n","    return max_tokenizer\n","\n","def get_new_context(context, num_sentences=5):\n","    context_puncs = find_sentence_locs(context)\n","    num_puncs = num_sentences + 1\n","    if len(context_puncs) \u003e= num_puncs:\n","      context = context[context_puncs[-num_puncs]:]\n","    return context\n","\n","def get_sequence(input, context, tokens):\n","    max_tokens = 510\n","    #build a string of user defined tokens\n","    tokens = \" \".join(tokens)\n","    max_tokenizer = get_max_tokenizer(input, context)\n","    context_input_tokenized = max_tokenizer.tokenize(context+\" \"+input)\n","    user_tokens_tokenized = gpt2_tokenizer.tokenize(tokens)\n","    if len(context_input_tokenized + user_tokens_tokenized) \u003e max_tokens:\n","        index_for_slice = max_tokens - len(user_tokens_tokenized)\n","        context_input_tokenized = context_input_tokenized[-index_for_slice:]\n","        tokens = gpt2_tokenizer.convert_tokens_to_string(user_tokens_tokenized)\n","        context_input = max_tokenizer.convert_tokens_to_string(context_input_tokenized)\n","        return  tokens + context_input, context_input\n","    else:\n","        context_input = context + \" \" + input\n","        return tokens + context_input, context_input\n","\n","def get_sentiment(sequence):\n","    sequence_max_score = 0\n","    max_sentiment = \"\"\n","    for d in sentiment_generator(sequence)[0]:\n","      if d['score'] \u003e sequence_max_score:\n","          sequence_max_score = d[\"score\"]\n","          max_sentiment = d[\"label\"]\n","    return int(max_sentiment.split(\" \")[0]), sequence_max_score\n","\n","def get_contradiction(sequence, output):\n","    sequence_tokens = contradiction_tokenizer.tokenize(sequence + \" \" + output)\n","    sequence = contradiction_tokenizer.convert_tokens_to_string(sequence_tokens[-510:])\n","    d = contradiction_generator(sequence)[0][0] \n","    return d[\"score\"] \n","\n","def clean_output(output):  \n","    output = output.replace(\"\\n\", \"\")\n","    output = output.replace(\"newline\u003e\", \"\")\n","    #round off to last sentence, if possible\n","    punc_locations = find_sentence_locs(output)\n","    output = output[:punc_locations[-1]] if len(punc_locations) \u003e 0 else output\n","    return output\n","\n","def get_accepted_outputs(sequence, outputs):\n","    sequence_sentiment, sequence_score = get_sentiment(sequence)\n","    accepted = []\n","    for output in outputs:\n","        if len(output) == 0:\n","            continue \n","        output = clean_output(output)\n","        output_sentiment, output_score = get_sentiment(output)\n","        if abs(output_sentiment - sequence_sentiment) \u003c= 1:\n","            output_contradiction = get_contradiction(sequence, output)\n","            if output_contradiction \u003c 0.8:\n","                accepted.append((output, output_score))\n","    return accepted\n","\n","def get_best_output(outputs):\n","    max = -1\n","    output = \"\"\n","    for sample, score in outputs:\n","        if score \u003e max:\n","            max = score\n","            output = sample\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gjg3z2RI0uHz"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 2/11 [00:42\u003c02:07, 14.14s/it]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9df0abcc9ec4ee3a688677e279221f5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=734.0, style=ProgressStyle(description_…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cef78e08cf0f4f01871728a05534c4d5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"927496d1e6eb426eb7b06b69e265c0b5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"625f9277560f4a32859318154810c68c","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=25.0, style=ProgressStyle(description_w…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\r 27%|██▋       | 3/11 [00:48\u003c01:32, 11.55s/it]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d3620d65369451ca902f6ee9d349ce6","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2243942751.0, style=ProgressStyle(descr…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"," 36%|███▋      | 4/11 [01:36\u003c02:38, 22.67s/it]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e39b0d952294601867e1751039c2279","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=953.0, style=ProgressStyle(description_…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66e9953ab317452a81c27c82cbefb200","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb30ee58783e4badb2a15b3c253f17be","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdde7ebbf86d4360ba0786cb6bdd2509","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=39.0, style=ProgressStyle(description_w…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\r 55%|█████▍    | 6/11 [01:40\u003c01:21, 16.40s/it]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b68cdbc39b3f468d99c2653f868c1292","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=669491321.0, style=ProgressStyle(descri…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\r 64%|██████▎   | 7/11 [02:09\u003c01:21, 20.29s/it]"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28ed3da2155341bc99644d98a42d82ea","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=688.0, style=ProgressStyle(description_…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7242d5fae4d24efd8b10e6e73587424f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73d076141de64afcb3179fbe639d69b1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83e8778f006a4014b497aa78103a1cd5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 9/11 [02:13\u003c00:29, 14.77s/it]"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25807b1f03574452b1c16562d4bc6e3c","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1425744429.0, style=ProgressStyle(descr…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 11/11 [02:43\u003c00:00, 14.83s/it]\n"]}],"source":["\n","# !wget https://console.cloud.google.com/storage/browser/general_photo_pdf_storage/gpt2_large_model/latest_model\n","# !wget https://console.cloud.google.com/storage/browser/general_photo_pdf_storage/gpt2_large_tokenizer/tokenizer-default\n","# -P \"/content/drive/My Drive/app\"\n","pbar = tqdm(total=11)\n","\n","GPT2_MODEL_PATH = './Models/gpt2_large_horror_generator/latest_model/'\n","GPT2_TOKENIZER_PATH = './Models/gpt2_large_horror_generator/tokenizer-default/'\n","gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_TOKENIZER_PATH,\n","                                               model_max_length=1024,\n","                                               padding_side='right')\n","gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n","\n","pbar.update(1)\n","gpt2_model = AutoModelWithLMHead.from_pretrained(GPT2_MODEL_PATH, \n","                                                 output_loading_info=False,\n","                                                 local_files_only=True,\n","                                                 pad_token_id=gpt2_tokenizer.eos_token_id).to(device)\n","pbar.update(1)\n","zeroshot_tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', verbose=False)\n","pbar.update(1)\n","zeroshot_model = AutoModelForSequenceClassification.from_pretrained(\"joeddav/xlm-roberta-large-xnli\", output_loading_info=False).to(device)\n","pbar.update(1)\n","zeroshot_generator = pipeline(\"zero-shot-classification\", device=0, model=zeroshot_model, tokenizer=zeroshot_tokenizer)\n","pbar.update(1)\n","\n","sentiment_tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", verbose=False)\n","pbar.update(1)\n","sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", output_loading_info=False).to(device)\n","pbar.update(1)\n","sentiment_generator = pipeline(\"sentiment-analysis\",model=sentiment_model, tokenizer=sentiment_tokenizer, return_all_scores=True, device=0)\n","pbar.update(1)\n","\n","contradiction_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\", verbose=False)\n","pbar.update(1)\n","contradiction_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\", output_loading_info=False).to(device)\n","pbar.update(1)\n","contradiction_generator = pipeline(\"sentiment-analysis\",model=contradiction_model, tokenizer=contradiction_tokenizer, device=0, return_all_scores=True)\n","pbar.update(1)\n","pbar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5b60CEZdIsJM"},"outputs":[],"source":["app = Flask(__name__, template_folder=\"WebApp/templates\", static_folder=\"WebApp/static\")\n","run_with_ngrok(app)   #starts ngrok when the app is run\n","\n","\n","@app.route(\"/\")\n","def home():\n","    author_inputs, genre_inputs = generate_inputs()\n","    blacklist_inputs = get_blacklist_inputs()\n","    return render_template(\"index.html\", author_inputs=author_inputs, genre_inputs=genre_inputs, blacklist_inputs=blacklist_inputs)\n","        \n","\n","@app.route(\"/settings\", methods=['POST'])\n","def settings():\n","    global CONTEXT, TEMPERATURE, GENERATION_LENGTH, BLACKLIST, USER_TOKENS\n","    data = request.get_json()\n","    USER_TOKENS = data['tokens']\n","    CONTEXT = data['context']\n","    BLACKLIST = data['blacklist']\n","    TEMPERATURE = data['temperature']\n","    GENERATION_LENGTH = data['generation_length']\n","    return json.dumps({'success':True}), 200, {'ContentType':'application/json'}\n","\n","@app.route(\"/generate\", methods=['POST'])\n","def generate():\n","    global CONTEXT, TEMPERATURE, GENERATION_LENGTH, BLACKLIST, USER_TOKENS\n","    data = request.get_json()\n","\n","    CONTEXT = data['context']\n","    if CONTEXT[:-1] != \" \": CONTEXT += \" \"\n","\n","    new_input = data['input']\n","    new_context = get_new_context(CONTEXT, num_sentences=5)\n","\n","    sequence, context_input = get_sequence(new_input, \n","                                           new_context, \n","                                           USER_TOKENS+classify_input_tokens(new_context))\n","\n","    bad_words_ids = get_bad_word_ids(bad_words=BLACKLIST)\n","    \n","    accepted_outputs = []\n","    sequence_length = len(gpt2_tokenizer.tokenize(sequence))\n","    while (len(accepted_outputs) \u003c 1):\n","        # Generate outputs (out1 = without tokens, out2 = with tokens)\n","        predicted_outputs = generate_outputs(sequence, \n","                                             context_input, \n","                                             bad_words_ids[:-1], \n","                                             num_sequences=3, \n","                                             top_k=150, \n","                                             top_p=0.99, \n","                                             temperature=float(TEMPERATURE), \n","                                             max_length=sequence_length+int(GENERATION_LENGTH))\n","      \n","        accepted_outputs = get_accepted_outputs(context_input, predicted_outputs)\n","\n","    output = get_best_output(accepted_outputs)\n","    output = new_input + \" \" + output\n","    output = \" \".join(output.split())\n","\n","    if output.find(CONTEXT) != -1: CONTEXT = output\n","    else: CONTEXT += output\n","    response = app.response_class(\n","        response=json.dumps({'context': CONTEXT}),\n","        status=200,\n","        mimetype='application/json'\n","    )\n","\n","    return response\n","\n","\n","def run():\n","    CONTEXT = ''\n","    TEMPERATURE = 0.75\n","    GENERATION_LENGTH = 20\n","    BLACKLIST = []\n","    USER_TOKENS = []\n","    bad_words_ids = get_bad_word_ids()\n","\n","    # Application is running on http://_________.ngrok.io  \n","    app.run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0w3jq7QPIsJO"},"outputs":[],"source":["run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZ22rBNs5_OZ"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"run_app.ipynb","version":""},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}